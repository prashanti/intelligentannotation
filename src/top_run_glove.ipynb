{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13004,
     "status": "ok",
     "timestamp": 1653686467628,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "V9Dc9AQXSnqg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for module in [\"tensorflow_addons\"]:\n",
    "    try:\n",
    "        __import__(module)\n",
    "    except ImportError:\n",
    "        !pip install {module}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3702,
     "status": "ok",
     "timestamp": 1653686471323,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "OUgDAXP9TiLs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Input, GRU, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, concatenate, SpatialDropout1D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.traversal.depth_first_search import dfs_tree\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1653686471324,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "B2eUNHQfTlQg"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"model_output\", \"Experiments\")\n",
    "GLOVE_EMB_FILE = os.path.join(BASE_DIR, \"Embedding\", \"glove.840B.300d.bin\")\n",
    "DATASET_LOC = os.path.join(DATA_DIR, \"model_input\", \"dataset\")\n",
    "direct_parent = os.path.join(DATA_DIR, \"GO_Category\", \"GO_DirectParents.tsv\")\n",
    "\n",
    "go_category = [\"GO:0008150\", \"GO:0005575\", \"GO:0003674\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2296,
     "status": "ok",
     "timestamp": 1653686473603,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "6YudD3Os-t4S",
    "outputId": "f8fb581d-6e55-4f29-b8ae-48aea1c0e376"
   },
   "outputs": [],
   "source": [
    "direct_data = pd.read_csv(direct_parent, delimiter=\"\\t\", names=['Child', 'Parent']).replace({\"_\": \":\"}, regex=True).drop(0).reset_index(drop=True)\n",
    "onto_digraph = nx.from_pandas_edgelist(direct_data, source='Child', target='Parent', create_using=nx.classes.digraph.DiGraph)\n",
    "print(\"Number of nodes:\", onto_digraph.number_of_nodes(), \"\\nNumber of edges:\", onto_digraph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4439,
     "status": "ok",
     "timestamp": 1653686478031,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "8mT-Xd0l-8DX"
   },
   "outputs": [],
   "source": [
    "subsumers = dict((i, list(set(np.array(dfs_tree(onto_digraph, i).edges()).flatten().tolist() + [i]) - set([\"owl:Thing\"]))) for i in onto_digraph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1653686478036,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "_P5E4AqgTwNj"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"weight\": 0.75,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 64,\n",
    "    \"activation\": 'softmax', \n",
    "    \"rdropout\": 0.3,\n",
    "    \"optimizer\": 'adamw',\n",
    "    \"loss\": 'sigfocalCE',\n",
    "    \"callbacks\": [\n",
    "                  {\"early_stop\": tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                                patience = 10, \n",
    "                                                verbose = 1,\n",
    "                                                restore_best_weights = True)},\n",
    "    ],\n",
    "    \"learning_rate_func\": 'cosinedecay',\n",
    "    \"max_len\": 71,\n",
    "    \"max_char_len\": 15,\n",
    "    \"min_sent_len\": 3,\n",
    "    \"project\": \"Intelligent_OA\",\n",
    "    \"extra_info\": \"GloVe, New Sim Score Scheme, inputs: Word(30D), POS(100D)\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"name\": \"GLOVE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3879,
     "status": "ok",
     "timestamp": 1653686495455,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "pq_sPqnyT0MM"
   },
   "outputs": [],
   "source": [
    "train_data = json.load(open(os.path.join(DATASET_LOC, \"train.json\"), \"r\"))\n",
    "train_data = [i for i in train_data if len(i['tokens']) >= config.get(\"min_sent_len\")]\n",
    "\n",
    "test_data = json.load(open(os.path.join(DATASET_LOC, \"test.json\"), \"r\"))\n",
    "test_data = [i for i in test_data if len(i['tokens']) >= config.get(\"min_sent_len\")]\n",
    "input_data = train_data + test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1653686496530,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "yV7x42DvT4dm"
   },
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    \"tokens\": [i['tokens'] for i in input_data],\n",
    "    \"tags\": [i['iob_tags'] for i in input_data],\n",
    "    \"pos\": [i['pos_tags'] for i in input_data],\n",
    "}\n",
    "assert (len(all_data['tokens']) == len(all_data['tags']) == len(all_data['pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1319,
     "status": "ok",
     "timestamp": 1653686499457,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "-XOEGjQxUG16",
    "outputId": "d2a355ac-69b1-4348-c4eb-d37b4cd60d85"
   },
   "outputs": [],
   "source": [
    "words = [\"PAD\"] + sorted(set([j for i in all_data['tokens'] for j in i] + [\"UNK\", \"O\"]) - set([\"PAD\"]))\n",
    "tags = [\"PAD\"] + sorted(set([j for i in all_data['tags'] for j in i] + [\"UNK\", \"O\"]) - set([\"PAD\"]))\n",
    "chars = [\"PAD\"] + sorted(set([j for i in words for j in i] + [\"UNK\", \"O\"]) - set([\"PAD\"]))\n",
    "pos = [\"PAD\"] + sorted(set([j for i in all_data['pos'] for j in i] + [\"UNK\", \"O\"]) - set([\"PAD\"]))\n",
    "\n",
    "n_words, n_tags, n_chars, n_pos = len(words), len(tags), len(chars), len(pos)\n",
    "\n",
    "\n",
    "print(\"\\nNumber of Observations:{0}\\nNumber of words:{1}\\nNumber of tags:{2}\\nNumber of characters: {3}\\nNumber of pos: {4}\".format(\n",
    "    len(all_data['tokens']), n_words, n_tags, n_chars, n_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1653686500701,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "_3UZe1pe4nMT",
    "outputId": "b9ac9f5b-e5a0-4ab3-d685-64f2671ae280"
   },
   "outputs": [],
   "source": [
    "set([i.replace(\"B-\", \"\").replace(\"I-\", \"\") for i in tags]) - set(subsumers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1653686501802,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "GW_sMt1JUQ4V"
   },
   "outputs": [],
   "source": [
    "word2idx = dict((i, idx) for idx, i in enumerate(words))\n",
    "idx2word = dict((v, k) for k, v in word2idx.items())\n",
    "\n",
    "tag2idx = dict((i, idx) for idx, i in enumerate(tags))\n",
    "idx2tag = dict((v, k) for k, v in tag2idx.items())\n",
    "\n",
    "char2idx = dict((i, idx) for idx, i in enumerate(chars))\n",
    "idx2char = dict((v, k) for k, v in char2idx.items())\n",
    "\n",
    "pos2idx = dict((i, idx) for idx, i in enumerate(pos))\n",
    "idx2pos = dict((v, k) for k, v in pos2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4649,
     "status": "ok",
     "timestamp": 1653686506449,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "7pIDf-iyUVUs"
   },
   "outputs": [],
   "source": [
    "Y_tags = [[tag2idx.get(i) for i in sent] for sent in all_data['tags']]\n",
    "Y_tags = pad_sequences(maxlen=config.get(\"max_len\"), sequences=Y_tags, value=tag2idx.get(\"PAD\"), padding='post', truncating='post', dtype='float16')\n",
    "Y_tags = to_categorical(Y_tags, num_classes=n_tags, dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1653686507538,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "mmAxm3RvUXXE"
   },
   "outputs": [],
   "source": [
    "def get_sim(term1, term2):\n",
    "    if \"GO\" in term1 and \"GO\" in term2:\n",
    "        term1 = term1.replace(\"B-\", \"\").replace(\"I-\", \"\")\n",
    "        term2 = term2.replace(\"B-\", \"\").replace(\"I-\", \"\")\n",
    "        t1 = set(subsumers.get(term1, term1))\n",
    "        t2 = set(subsumers.get(term2, term2))\n",
    "        if len(set.union(t1, t2)) > 0:\n",
    "            simj=len(set.intersection(t1, t2))/len(set.union(t1, t2))\n",
    "        else:\n",
    "            simj = 0.0\n",
    "    else:\n",
    "        simj = 0.0\n",
    "    return simj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1626,
     "status": "ok",
     "timestamp": 1653686510960,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "khfXMDr0Ueeg",
    "outputId": "84289a8c-1f7b-44e0-9777-bb24188cf307"
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 68444,
     "status": "ok",
     "timestamp": 1653686579401,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "hsTNpkMPpcKN"
   },
   "outputs": [],
   "source": [
    "sem_dist = dict([(i, to_categorical(i, num_classes=n_tags)) for i in range(n_tags)])\n",
    "factor = 0\n",
    "for i in range(n_tags):\n",
    "    iob_i = None\n",
    "    term_i = idx2tag.get(i)\n",
    "    if \"B-\" in term_i or \"I-\" in term_i:\n",
    "        iob_i = term_i[0]\n",
    "    term_i = term_i.replace(\"B-\", \"\").replace(\"I-\", \"\")\n",
    "    if \"GO\" in term_i:\n",
    "        sem_scores = []\n",
    "        for j in range(n_tags):\n",
    "            iob_j = None\n",
    "            term_j = idx2tag.get(j)\n",
    "            if \"B-\" in term_j or \"I-\" in term_j:\n",
    "                iob_j = term_j[0]\n",
    "            term_j = term_j.replace(\"B-\", \"\").replace(\"I-\", \"\")\n",
    "            score = config.get('weight') * get_sim(term_i, term_j)\n",
    "            if iob_i != iob_j:\n",
    "                score = factor * score\n",
    "            sem_scores.append(score)\n",
    "        sem_scores = np.array(sem_scores)\n",
    "        # if config.get(\"weight\"):\n",
    "        #     unique_vals = np.unique(sem_scores)\n",
    "        sem_scores[i] = 1\n",
    "        sem_dist[i] = sem_scores\n",
    "\n",
    "for i in range(n_tags):\n",
    "    num_max = np.where(sem_dist[i] == 1)[0].size\n",
    "    assert num_max == 1\n",
    "\n",
    "for i in range(len(Y_tags)):\n",
    "    for j in range(config.get(\"max_len\")):\n",
    "        k = np.where(Y_tags[i][j] == 1)[0][0]\n",
    "        Y_tags[i][j] = sem_dist[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1123,
     "status": "ok",
     "timestamp": 1653686581812,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "qY4oR2cEUhKM"
   },
   "outputs": [],
   "source": [
    "for k, v in sem_dist.items():\n",
    "    try:\n",
    "        assert np.where(v == 1)[0].size == 1\n",
    "    except:\n",
    "        print(np.where(v == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17047,
     "status": "ok",
     "timestamp": 1653686598854,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "92YnWFoaUkCq"
   },
   "outputs": [],
   "source": [
    "X_word = [[word2idx.get(w) for w in s] for s in all_data['tokens']]\n",
    "X_word = pad_sequences(maxlen=config.get(\"max_len\"), sequences=X_word, value=word2idx[\"PAD\"], padding='post', truncating='post', dtype='float16')\n",
    "\n",
    "X_char_temp = []\n",
    "for wds in all_data['tokens']:\n",
    "    wds = wds[:config.get(\"max_len\")] + [\"PAD\"]*(config.get(\"max_len\") - len(wds))\n",
    "    chrs = [list(word)[:config.get(\"max_char_len\")] + [\"PAD\"]*(config.get(\"max_char_len\")-len(word)) if word != \"PAD\" \n",
    "            else [\"PAD\"]*config.get(\"max_char_len\") for word in wds]\n",
    "    X_char_temp.append(np.array(chrs))\n",
    "X_char_temp = np.array(X_char_temp)\n",
    "X_char = np.vectorize(char2idx.get)(X_char_temp).astype('float16')\n",
    "del X_char_temp\n",
    "\n",
    "X_pos = [[pos2idx.get(w) for w in s] for s in all_data['pos']]\n",
    "X_pos = pad_sequences(maxlen=config.get(\"max_len\"), sequences=X_pos, value=pos2idx.get(\"PAD\"), padding='post', truncating='post', dtype='float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2711,
     "status": "ok",
     "timestamp": 1653686601558,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "qESjrT69Urag"
   },
   "outputs": [],
   "source": [
    "max_idx = 0\n",
    "for i in range(len(X_word), 0, -1):\n",
    "    if (i*0.7%config.get(\"batch_size\") == 0) and (i*0.3%config.get(\"batch_size\") == 0):\n",
    "        max_idx = i\n",
    "        break\n",
    "\n",
    "combined = [(X_word[i], X_char[i], X_pos[i]) for i in range(max_idx)]\n",
    "Y_tags = Y_tags[:max_idx]\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(combined, Y_tags, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1475,
     "status": "ok",
     "timestamp": 1653686603028,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "Sjx5s0gOUwAr"
   },
   "outputs": [],
   "source": [
    "input_train = []\n",
    "for i in range(len(combined[0])):\n",
    "    temp = []\n",
    "    for j in range(len(X_tr)):\n",
    "        temp.append(X_tr[j][i])\n",
    "    input_train.append(np.array(temp, dtype='float16'))\n",
    "input_train = tuple(input_train)\n",
    "\n",
    "input_test = []\n",
    "for i in range(len(combined[0])):\n",
    "    temp = []\n",
    "    for j in range(len(X_te)):\n",
    "        temp.append(X_te[j][i])\n",
    "    input_test.append(np.array(temp, dtype='float16'))\n",
    "input_test = tuple(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1653686603931,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "KqbG9awb5R4O",
    "outputId": "ad5fb295-34b9-473b-be37-801b6d25ed95"
   },
   "outputs": [],
   "source": [
    "[i.shape for i in input_train], y_tr.shape, [i.shape for i in input_test], y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11034,
     "status": "ok",
     "timestamp": 1653686614962,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "NoYWwIxfUyTP"
   },
   "outputs": [],
   "source": [
    "X_train_dataset = tf.data.Dataset.from_tensor_slices(input_train)\n",
    "X_test_dataset = tf.data.Dataset.from_tensor_slices(input_test)\n",
    "Y_train_dataset = tf.data.Dataset.from_tensor_slices(y_tr)\n",
    "Y_test_dataset = tf.data.Dataset.from_tensor_slices(y_te)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((X_train_dataset, Y_train_dataset))\n",
    "test_dataset = tf.data.Dataset.zip((X_test_dataset, Y_test_dataset))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000).batch(config['batch_size'])\n",
    "test_dataset = test_dataset.batch(config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1051,
     "status": "ok",
     "timestamp": 1653686616009,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "QCeLnpSgUze3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def go_val_acc(y_true, y_pred):\n",
    "    tmp = tf.math.argmax(y_pred[0], axis=-1)\n",
    "    tmp_g = tf.math.argmax(y_true[0], axis=-1)\n",
    "    #mask 0\n",
    "    tmp_m = tf.math.equal(tmp_g, tf.constant(0, dtype=tf.int64))\n",
    "    tmp_g_m = tf.math.equal(tmp, tf.constant(0, dtype=tf.int64))\n",
    "    \n",
    "    pr = tmp[tf.math.logical_not(tf.math.logical_and(tmp_m, tmp_g_m))]\n",
    "    gr = tmp_g[tf.math.logical_not(tf.math.logical_and(tmp_m, tmp_g_m))]\n",
    "    \n",
    "    if pr is not None:\n",
    "        c = tf.math.reduce_sum(tf.cast(pr==gr, dtype=tf.int64))/tf.cast(tf.size(pr), dtype=tf.int64)\n",
    "        return c\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def go_val_f1(y_true, y_pred):\n",
    "    tmp_p = tf.math.argmax(y_pred[0], axis=-1)\n",
    "    tmp_g = tf.math.argmax(y_true[0], axis=-1)\n",
    "    #mask 0\n",
    "    tmp_p_m = tf.math.equal(tmp_p, tf.constant(0, dtype=tf.int64))\n",
    "    tmp_g_m = tf.math.equal(tmp_g, tf.constant(0, dtype=tf.int64))\n",
    "    \n",
    "    #get the positions where 0 != 0\n",
    "    pr = y_pred[tf.math.logical_not(tf.math.logical_and(tmp_p_m, tmp_g_m))]\n",
    "    gr = y_true[tf.math.logical_not(tf.math.logical_and(tmp_p_m, tmp_g_m))]\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(gr * pr, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(gr, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(pr, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1653686617150,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "tWvn9dg9U1DT"
   },
   "outputs": [],
   "source": [
    "def decayed_learning_rate(step):\n",
    "    step = min(step, decay_steps)\n",
    "    cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n",
    "    decayed = (1 - alpha) * cosine_decay + alpha\n",
    "    return initial_learning_rate * decayed\n",
    "\n",
    "def get_lr(lr_algo):\n",
    "    if lr_algo == 'cosinedecay':\n",
    "        decay_steps = 1000\n",
    "        return tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate, decay_steps)\n",
    "    elif lr_algo == 'cosinedecayrst':\n",
    "        decay_steps = 1000\n",
    "        return tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate, decay_steps)\n",
    "                   \n",
    "\n",
    "def get_optimizer(opt, lr):\n",
    "    if opt == 'adam':\n",
    "#         return tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        return tf.keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "    elif opt == 'adamw':\n",
    "        step = tf.Variable(0, trainable=False)\n",
    "        schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "            [10000, 15000], [1e-0, 1e-1, 1e-2])\n",
    "        # lr and wd can be a function or a tensor\n",
    "        lr = lr * schedule(step)\n",
    "        wd = lambda: 1e-4 * schedule(step)\n",
    "        return tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "    elif opt == 'rmsprop':\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    \n",
    "def get_loss(loss):\n",
    "    if 'categoricalCE' in loss:\n",
    "        if 'logits' in loss:\n",
    "            return tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        else:\n",
    "            return tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    elif 'sigfocalCE' in loss:\n",
    "        if 'logits' in loss:\n",
    "            return tfa.losses.SigmoidFocalCrossEntropy(from_logits=True),\n",
    "        else:\n",
    "            return tfa.losses.SigmoidFocalCrossEntropy(from_logits=False),\n",
    "    elif 'crf' in loss: # does not work with distribute stratergy\n",
    "        crf = CRF(n_tags+1)\n",
    "        return crf.loss_function,\n",
    "    elif 'sparsecatCE' in loss:\n",
    "        if 'logits' in loss:\n",
    "            return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        else:\n",
    "            return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "    elif 'kl' in loss:\n",
    "        return tf.keras.losses.KLDivergence(),\n",
    "    elif loss == 'binaryCE':\n",
    "        if 'logits' in loss:\n",
    "            return tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        else:\n",
    "            return tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    elif 'mse' in loss:\n",
    "        return tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30817,
     "status": "ok",
     "timestamp": 1653686647964,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "a5sweoJPNDOJ",
    "outputId": "b033ba02-07d3-4865-d215-ced56ed9ba29"
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(GLOVE_EMB_FILE, binary=True)\n",
    "w2v_vocab, w2v_vector = word2vec.vocab, word2vec.vectors\n",
    "embeddings_index = dict((vocab, vector) for vocab, vector in zip(w2v_vocab, w2v_vector))\n",
    "emb_matrix = np.zeros((n_words, 300))\n",
    "hits, misses = 0, 0\n",
    "for word, index in word2idx.items():\n",
    "    vec = embeddings_index.get(word)\n",
    "    if vec is not None:\n",
    "        emb_matrix[index] = vec\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Total hit: {0}\\nTotal miss: {1}\".format(hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2022,
     "status": "ok",
     "timestamp": 1653686649980,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "zjKcp4zJNHSD",
    "outputId": "7295e8dd-56dc-441f-af6e-8a386f709454"
   },
   "outputs": [],
   "source": [
    "# input and embedding for words\n",
    "word_in = Input(shape=(config.get(\"max_len\"),), name=\"WORD\")\n",
    "emb_word = Embedding(input_dim=n_words, output_dim=300, embeddings_initializer=tf.keras.initializers.Constant(emb_matrix),\n",
    "                              input_length=config.get(\"max_len\"), trainable=True, name=\"GloVe\")(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(config.get(\"max_len\"), config.get(\"max_char_len\"),), name=\"CHAR\")\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=100,\n",
    "                           input_length=config.get(\"max_char_len\"), mask_zero=True))(char_in)\n",
    "\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(GRU(units=150, return_sequences=False,\n",
    "                                recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "#input and embeddings for pos\n",
    "pos_in = Input(shape=(config.get(\"max_len\"),), name=\"POS\")\n",
    "emb_pos = Embedding(input_dim=n_pos, output_dim=100, input_length=config.get(\"max_len\"), mask_zero=True, name=\"EMB_POS\")(pos_in)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc, emb_pos])\n",
    "x = SpatialDropout1D(config.get(\"dropout\"))(x)\n",
    "main_lstm = Bidirectional(GRU(units=150, return_sequences=True,\n",
    "                               recurrent_dropout=config['rdropout']))(x)\n",
    "main_lstm = TimeDistributed(Dense(3200, activation='relu'))(main_lstm)\n",
    "main_lstm = Dropout(config.get(\"dropout\"))(main_lstm)\n",
    "out = TimeDistributed(Dense(n_tags, activation=config['activation']))(main_lstm)\n",
    "\n",
    "model = Model([word_in, char_in, pos_in], out)\n",
    "\n",
    "model.compile(optimizer = get_optimizer(config['optimizer'], config['learning_rate']), \n",
    "              loss=get_loss(config['loss']), metrics=[\"acc\", go_val_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1286,
     "status": "ok",
     "timestamp": 1653686651259,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "vzLeuN1QU3Yz",
    "outputId": "a33d3f9f-a1e7-4ba8-b60d-8ea670a15dcb"
   },
   "outputs": [],
   "source": [
    "# input and embedding for words\n",
    "word_in = Input(shape=(config.get(\"max_len\"),), name=\"WORD\")\n",
    "emb_word = Embedding(input_dim=n_words, output_dim=30,\n",
    "                     input_length=config.get(\"max_len\"), mask_zero=True)(word_in)\n",
    "\n",
    "# input and embeddings for characters\n",
    "char_in = Input(shape=(config.get(\"max_len\"), config.get(\"max_char_len\"),), name=\"CHAR\")\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=100,\n",
    "                           input_length=config.get(\"max_char_len\"), mask_zero=True))(char_in)\n",
    "\n",
    "# character LSTM to get word encodings by characters\n",
    "char_enc = TimeDistributed(GRU(units=150, return_sequences=False,\n",
    "                                recurrent_dropout=0.5))(emb_char)\n",
    "\n",
    "#input and embeddings for pos\n",
    "pos_in = Input(shape=(config.get(\"max_len\"),), name=\"POS\")\n",
    "emb_pos = Embedding(input_dim=n_pos, output_dim=100, input_length=config.get(\"max_len\"), mask_zero=True, name=\"EMB_POS\")(pos_in)\n",
    "\n",
    "# main LSTM\n",
    "x = concatenate([emb_word, char_enc, emb_pos])\n",
    "# x = concatenate([emb_word, char_enc, emb_pos, prot_in, biom_in, chem_in])\n",
    "x = SpatialDropout1D(config.get(\"dropout\"))(x)\n",
    "main_lstm = Bidirectional(GRU(units=150, return_sequences=True,\n",
    "                               recurrent_dropout=config['rdropout']))(x)\n",
    "main_lstm = TimeDistributed(Dense(3200, activation='relu'))(main_lstm)\n",
    "main_lstm = Dropout(config.get(\"dropout\"))(main_lstm)\n",
    "out = TimeDistributed(Dense(n_tags, activation=config['activation']))(main_lstm)\n",
    "\n",
    "model = Model([word_in, char_in, pos_in], out)\n",
    "\n",
    "model.compile(optimizer = get_optimizer(config['optimizer'], config['learning_rate']), \n",
    "              loss=get_loss(config['loss']), metrics=[\"acc\", go_val_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1653686652136,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "A0XiPg-NU5js",
    "outputId": "8313a4fa-f6d8-4228-ecb9-a3698b353b1a"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "executionInfo": {
     "elapsed": 2709,
     "status": "ok",
     "timestamp": 1653686654839,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "1jBCYjJfU7-i",
    "outputId": "fa66a80c-7d1d-4c23-ffe4-1f385a5fa3cf"
   },
   "outputs": [],
   "source": [
    "model_arch = tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "display(model_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7061550,
     "status": "ok",
     "timestamp": 1653694398243,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "0rjyIW0BU9Pt",
    "outputId": "4186651e-75d3-409e-e445-43fb5154b010"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    batch_size=config['batch_size'],\n",
    "                    epochs=config['epochs'], \n",
    "                    validation_data=test_dataset,\n",
    "                    verbose=1,\n",
    "                    callbacks=[v for i in config.get(\"callbacks\") for v in i.values()]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 5329,
     "status": "ok",
     "timestamp": 1653694403574,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "MioUL3y2U-bX",
    "outputId": "b93b72fb-36fb-44ed-c5af-233625969a7b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,5)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g.', linestyle='solid', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', linestyle='solid', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(\"loss.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 5430,
     "status": "ok",
     "timestamp": 1653694409005,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "JAMlw-29VAC5",
    "outputId": "267bfceb-3640-48f7-d342-29b9bb7e068e"
   },
   "outputs": [],
   "source": [
    "# graph of mean absolute error\n",
    "mae = history.history['acc']\n",
    "val_mae = history.history['val_acc']\n",
    "plt.plot(epochs, mae, 'g.', linestyle='solid', label='Training Accuracy')\n",
    "plt.plot(epochs, val_mae, 'b.', linestyle='solid', label='Validation Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(\"acc.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4924,
     "status": "ok",
     "timestamp": 1653694413930,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "vMKAXkAnVBSf"
   },
   "outputs": [],
   "source": [
    "del X_word, X_char, X_pos, Y_tags, combined, input_train, X_train_dataset, Y_train_dataset, train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCPdmyp0VP0b"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "pbar = tqdm(total=len(X_te), desc=\"Making predictions:\")\n",
    "for i in range(int(len(X_te)/10)):\n",
    "    # temp = np.argsort(-1*model.predict([j[i*10:(i+1)*10] for j in input_test]), axis=-1)\n",
    "    inp = [j[i*10:(i+1)*10] for j in input_test]\n",
    "    if inp[0].shape[0] != 0:\n",
    "        temp = model.predict(inp)\n",
    "    pred.append(temp)\n",
    "    pbar.update(temp.shape[0])\n",
    "pred = np.concatenate(pred)\n",
    "pbar.close()\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv6MKC08SwtB"
   },
   "outputs": [],
   "source": [
    "word = np.vectorize(idx2word.get)(input_test[0]).flatten()\n",
    "ground_truth = np.vectorize(idx2tag.get)(np.argmax(y_te, axis=-1)).flatten()\n",
    "predictions = (np.vectorize(idx2tag.get)(np.argmax(pred, axis=-1))).flatten().tolist()\n",
    "two_predictions = (np.vectorize(idx2tag.get)(np.argsort(-1*pred, axis=-1)[:,:,:2])).reshape(pred.shape[0]*pred.shape[1], 2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAL3ZEXSVXUd"
   },
   "outputs": [],
   "source": [
    "pd_data = pd.DataFrame({\n",
    "    \"Word\": word,\n",
    "    \"Ground_Truth\": ground_truth,\n",
    "    \"Prediction\": predictions,\n",
    "    \"Top_Two_Predictions\": two_predictions,\n",
    "})\n",
    "pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTy4tiyJVYZT"
   },
   "outputs": [],
   "source": [
    "pd_data.drop(pd_data[pd_data['Ground_Truth'] == \"PAD\"].index, inplace=True)\n",
    "pd_data['Comparison'] = pd_data.apply(lambda x: x[1] if x[1] in x[-1] else x[-1][-1], axis=1)\n",
    "pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 6612,
     "status": "ok",
     "timestamp": 1653694667580,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "AXflVC_S0fYl",
    "outputId": "62f0ce72-3ad8-4b3e-fcee-d5f6e8bd1b59"
   },
   "outputs": [],
   "source": [
    "pd_data.drop(pd_data[(pd_data[\"Ground_Truth\"] == \"O\") & (pd_data[\"Prediction\"] == \"O\")].index, inplace=True)\n",
    "pd_data.drop(pd_data[(pd_data[\"Ground_Truth\"] == \"EOS\") & (pd_data[\"Prediction\"] == \"EOS\")].index, inplace=True)\n",
    "pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j8bez660gRp"
   },
   "outputs": [],
   "source": [
    "top_report = classification_report(\n",
    "    pd_data['Ground_Truth'],\n",
    "    pd_data['Prediction'],\n",
    "    zero_division=False,\n",
    "    digits = 4\n",
    ")\n",
    "print(top_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-6zykJWE3WZ"
   },
   "outputs": [],
   "source": [
    "{\"IOB_F1\": top_report.splitlines()[-1].split()[-2],\n",
    "                      \"IOB_Sim\": np.round(pd_data[['Prediction','Ground_Truth']].apply(lambda x: get_sim(x[0], x[1]), axis=1).mean(), 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8hQSO-BE7n0"
   },
   "outputs": [],
   "source": [
    "df1 = pd_data.copy().replace({\"B-GO:\": \"GO:\", \"I-GO:\": \"GO:\"}, regex=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5258,
     "status": "ok",
     "timestamp": 1653694693637,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "CQOkeqavE9PI",
    "outputId": "d5e8cc3f-b141-41bf-9fd9-6d341f5b409c"
   },
   "outputs": [],
   "source": [
    "report = classification_report(\n",
    "    df1['Ground_Truth'],\n",
    "    df1['Prediction'],\n",
    "    zero_division=False,\n",
    "    digits = 4\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7Gza6WwE-e_"
   },
   "outputs": [],
   "source": [
    "{\"F1\": report.splitlines()[-1].split()[-2],\n",
    "                      \"Sim\": np.round(df1[['Prediction','Ground_Truth']].apply(lambda x: get_sim(x[0], x[1]), axis=1).mean(), 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y18691rLFDSs"
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "df2.drop(df2[(df2[\"Ground_Truth\"] == \"O\") & (df2[\"Comparison\"] == \"O\")].index, inplace=True)\n",
    "df2.drop(df2[(df2[\"Ground_Truth\"] == \"EOS\") & (df2[\"Comparison\"] == \"EOS\")].index, inplace=True)\n",
    "wb_run.log({\"top_two_dataframe\": wandb.Table(dataframe=df2)})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIwuF2bhFEXw"
   },
   "outputs": [],
   "source": [
    "top_two_report = classification_report(\n",
    "    df2['Ground_Truth'],\n",
    "    df2['Comparison'],\n",
    "    zero_division=False,\n",
    "    digits = 4\n",
    ")\n",
    "print(top_two_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4989,
     "status": "ok",
     "timestamp": 1653694720790,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "11IMQtpHFFcT",
    "outputId": "990461f9-7d72-476a-fc3b-35575aef35d5"
   },
   "outputs": [],
   "source": [
    "{\"F1_Top_2\": top_two_report.splitlines()[-1].split()[-2],\n",
    "                      \"Sim_Top_2\": np.round(df2[['Comparison','Ground_Truth']].apply(lambda x: get_sim(x[0], x[1]), axis=1).mean(), 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1653694736088,
     "user": {
      "displayName": "NLP Onto",
      "userId": "16773177303145403854"
     },
     "user_tz": 240
    },
    "id": "3rR4KFvbFKs8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPqeaetnulkGBUooG283oG4",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1trlh7zGzk5GuMH1klEvC3senUE6e25Ru",
   "name": "top_run_glove.ipynb",
   "provenance": [
    {
     "file_id": "1w1wMMm2eck4r6i_UCuBz0ucTM9TWXyC5",
     "timestamp": 1653632775329
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
